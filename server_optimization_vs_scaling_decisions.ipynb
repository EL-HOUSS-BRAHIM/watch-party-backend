{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906ff634",
   "metadata": {},
   "source": [
    "# Server Optimization vs Scaling: Decision Framework\n",
    "\n",
    "This notebook provides a systematic approach to deciding when to optimize your existing resources versus when to scale up your infrastructure for Django applications like WatchParty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17044c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For system monitoring\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38a07c",
   "metadata": {},
   "source": [
    "## 2. Key Metrics to Monitor\n",
    "\n",
    "Before making any decision about optimization vs. scaling, you need to collect and analyze key performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function to collect system metrics\n",
    "def collect_system_metrics():\n",
    "    metrics = {\n",
    "        'cpu_percent': psutil.cpu_percent(interval=1),\n",
    "        'memory_percent': psutil.virtual_memory().percent,\n",
    "        'swap_percent': psutil.swap_memory().percent,\n",
    "        'disk_percent': psutil.disk_usage('/').percent,\n",
    "        'load_avg': psutil.getloadavg()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Example: Collect metrics once\n",
    "current_metrics = collect_system_metrics()\n",
    "print(\"Current System Metrics:\")\n",
    "for key, value in current_metrics.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e1a6e",
   "metadata": {},
   "source": [
    "## 3. Decision Framework: When to Optimize vs. Scale\n",
    "\n",
    "Let's create a decision framework based on the metrics we collect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample decision framework\n",
    "def analyze_resource_needs(metrics, thresholds={}):\n",
    "    \n",
    "    # Default thresholds if not provided\n",
    "    default_thresholds = {\n",
    "        'cpu_percent': {'optimize': 70, 'scale': 85},\n",
    "        'memory_percent': {'optimize': 75, 'scale': 90},\n",
    "        'swap_percent': {'optimize': 50, 'scale': 70},\n",
    "        'disk_percent': {'optimize': 70, 'scale': 90},\n",
    "    }\n",
    "    \n",
    "    # Use provided thresholds or defaults\n",
    "    t = {k: thresholds.get(k, v) for k, v in default_thresholds.items()}\n",
    "    \n",
    "    recommendations = {}\n",
    "    \n",
    "    # Analyze each metric\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in t:\n",
    "            if value >= t[metric]['scale']:\n",
    "                recommendations[metric] = {'action': 'SCALE', 'value': value, 'threshold': t[metric]['scale']}\n",
    "            elif value >= t[metric]['optimize']:\n",
    "                recommendations[metric] = {'action': 'OPTIMIZE', 'value': value, 'threshold': t[metric]['optimize']}\n",
    "            else:\n",
    "                recommendations[metric] = {'action': 'MONITOR', 'value': value}\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "recommendations = analyze_resource_needs(current_metrics)\n",
    "for metric, rec in recommendations.items():\n",
    "    print(f\"{metric}: {rec['action']} (Current: {rec['value']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f472a",
   "metadata": {},
   "source": [
    "## 4. Optimization Techniques for Django Applications\n",
    "\n",
    "Before scaling, consider these optimization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1984bf",
   "metadata": {},
   "source": [
    "### 4.1 Database Optimization\n",
    "\n",
    "- **Identify slow queries**: Use Django Debug Toolbar or query logging\n",
    "- **Add indexes**: For frequently queried fields\n",
    "- **Implement query caching**: With Redis or Memcached\n",
    "- **Use select_related/prefetch_related**: To reduce query count\n",
    "\n",
    "Example implementation for analyzing Django queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c66cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function to parse Django query logs\n",
    "def analyze_query_log(log_file_path):\n",
    "    # This is a simplified example - in practice you'd parse actual log files\n",
    "    sample_queries = [\n",
    "        {'query': 'SELECT * FROM users_profile', 'time': 0.05, 'count': 250},\n",
    "        {'query': 'SELECT * FROM videos WHERE user_id=?', 'time': 0.3, 'count': 120},\n",
    "        {'query': 'SELECT * FROM parties LEFT JOIN users', 'time': 1.2, 'count': 45},\n",
    "    ]\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    query_df = pd.DataFrame(sample_queries)\n",
    "    \n",
    "    # Calculate total time per query type\n",
    "    query_df['total_time'] = query_df['time'] * query_df['count']\n",
    "    \n",
    "    # Sort by most expensive queries\n",
    "    return query_df.sort_values('total_time', ascending=False)\n",
    "\n",
    "# Example usage\n",
    "query_analysis = analyze_query_log('django_queries.log')\n",
    "query_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6879f2e",
   "metadata": {},
   "source": [
    "### 4.2 Memory Optimization\n",
    "\n",
    "- **Gunicorn worker tuning**: Adjust worker count based on cores and available memory\n",
    "- **Memory limits**: Set in systemd service files\n",
    "- **Middleware evaluation**: Remove unnecessary middleware\n",
    "- **Cache optimization**: Redis connection pooling and key expiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a95f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate optimal Gunicorn workers\n",
    "def calculate_gunicorn_workers(cpu_count=None, memory_gb=None):\n",
    "    if cpu_count is None:\n",
    "        cpu_count = psutil.cpu_count()\n",
    "    \n",
    "    if memory_gb is None:\n",
    "        memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    \n",
    "    # CPU-based calculation (2-4 Ã— cores)\n",
    "    cpu_based = min(2 * cpu_count + 1, 4 * cpu_count)\n",
    "    \n",
    "    # Memory-based calculation (assume 250MB per worker)\n",
    "    memory_based = int((memory_gb * 0.75) / 0.25)  # Use 75% of memory, assume 250MB per worker\n",
    "    \n",
    "    # Take the minimum of the two\n",
    "    optimal_workers = min(cpu_based, memory_based)\n",
    "    \n",
    "    return {\n",
    "        'cpu_based': cpu_based,\n",
    "        'memory_based': memory_based,\n",
    "        'optimal_workers': optimal_workers,\n",
    "        'worker_memory_mb': (memory_gb * 1024 * 0.75) / optimal_workers\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "worker_recommendation = calculate_gunicorn_workers()\n",
    "print(f\"Recommended Gunicorn Workers: {worker_recommendation['optimal_workers']}\")\n",
    "print(f\"Estimated Memory per Worker: {worker_recommendation['worker_memory_mb']:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4aae01",
   "metadata": {},
   "source": [
    "### 4.3 Cache Optimization\n",
    "\n",
    "- **View caching**: Cache entire views or fragments\n",
    "- **Template fragment caching**: Cache portions of templates\n",
    "- **Low-level API caching**: Cache API responses with varying TTLs\n",
    "- **Use Redis for session storage**: Better performance than database sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze cache hit rates\n",
    "def analyze_cache_performance(days=7):\n",
    "    # Sample data - in practice, get this from monitoring tools\n",
    "    dates = pd.date_range(end=datetime.now(), periods=days)\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    cache_data = {\n",
    "        'date': dates,\n",
    "        'hit_rate': np.random.uniform(0.6, 0.9, days),\n",
    "        'miss_rate': np.random.uniform(0.1, 0.4, days),\n",
    "        'request_count': np.random.randint(5000, 15000, days)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(cache_data)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot hit/miss rates\n",
    "    ax1.plot(df['date'], df['hit_rate'], 'g-', label='Hit Rate')\n",
    "    ax1.plot(df['date'], df['miss_rate'], 'r-', label='Miss Rate')\n",
    "    ax1.set_ylabel('Rate')\n",
    "    ax1.set_title('Cache Hit vs Miss Rate')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot request count\n",
    "    ax2.bar(df['date'], df['request_count'], color='skyblue')\n",
    "    ax2.set_ylabel('Request Count')\n",
    "    ax2.set_title('Daily Request Count')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate average hit rate\n",
    "    avg_hit_rate = df['hit_rate'].mean()\n",
    "    print(f\"Average cache hit rate: {avg_hit_rate:.2%}\")\n",
    "    \n",
    "    if avg_hit_rate < 0.7:\n",
    "        print(\"RECOMMENDATION: Improve cache strategy - hit rate below 70%\")\n",
    "    else:\n",
    "        print(\"RECOMMENDATION: Cache performance acceptable\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "cache_analysis = analyze_cache_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5701f2",
   "metadata": {},
   "source": [
    "## 5. When to Scale - Clear Indicators\n",
    "\n",
    "Here are the key indicators that it's time to scale rather than optimize:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b4fdd",
   "metadata": {},
   "source": [
    "### 5.1 Persistent High Resource Usage Despite Optimization\n",
    "\n",
    "If you've implemented optimizations but still see:\n",
    "\n",
    "- **CPU**: Consistently >85% utilization during normal traffic\n",
    "- **Memory**: Consistently >90% usage with swapping\n",
    "- **Disk I/O**: Persistent high wait times\n",
    "- **Network**: Bandwidth saturation\n",
    "\n",
    "### 5.2 Response Time Degradation\n",
    "\n",
    "- **Increasing response times**: Even after optimization\n",
    "- **Timeout errors**: Appearing during peak traffic\n",
    "- **Error rates**: Increasing under load\n",
    "\n",
    "### 5.3 Traffic Growth Patterns\n",
    "\n",
    "- **Sustained growth**: Not just temporary spikes\n",
    "- **Predictable peaks**: That exceed current capacity\n",
    "- **New features**: That will increase per-user resource requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze response time trends\n",
    "def analyze_response_times(days=30):\n",
    "    # Sample data - in practice, get this from monitoring tools\n",
    "    dates = pd.date_range(end=datetime.now(), periods=days)\n",
    "    \n",
    "    # Generate sample data with an upward trend\n",
    "    np.random.seed(42)\n",
    "    base = np.linspace(100, 350, days)  # Increasing baseline from 100ms to 350ms\n",
    "    variation = np.random.normal(0, 50, days)  # Add some noise\n",
    "    p95_factor = 2.5  # p95 is 2.5x the average\n",
    "    p99_factor = 5  # p99 is 5x the average\n",
    "    \n",
    "    response_data = {\n",
    "        'date': dates,\n",
    "        'avg_response_ms': base + variation,\n",
    "        'p95_response_ms': (base + variation) * p95_factor + np.random.normal(0, 100, days),\n",
    "        'p99_response_ms': (base + variation) * p99_factor + np.random.normal(0, 200, days),\n",
    "        'error_rate': np.clip(np.random.normal(0.01, 0.005, days) + np.linspace(0, 0.02, days), 0, 1),\n",
    "        'daily_users': np.random.randint(1000, 2000, days) + np.linspace(0, 1000, days).astype(int)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(response_data)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Plot response times\n",
    "    ax1.plot(df['date'], df['avg_response_ms'], 'b-', label='Avg Response Time')\n",
    "    ax1.plot(df['date'], df['p95_response_ms'], 'orange', label='p95 Response Time')\n",
    "    ax1.plot(df['date'], df['p99_response_ms'], 'r-', label='p99 Response Time')\n",
    "    ax1.axhline(y=300, color='r', linestyle='--', alpha=0.7, label='Target SLA (300ms)')\n",
    "    ax1.set_ylabel('Response Time (ms)')\n",
    "    ax1.set_title('API Response Time Trends')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot error rate\n",
    "    ax2.plot(df['date'], df['error_rate'] * 100, 'r-')\n",
    "    ax2.set_ylabel('Error Rate (%)')\n",
    "    ax2.set_title('API Error Rate')\n",
    "    ax2.axhline(y=1, color='orange', linestyle='--', alpha=0.7, label='Target (1%)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot user count\n",
    "    ax3.bar(df['date'], df['daily_users'], color='skyblue')\n",
    "    ax3.set_ylabel('Daily Active Users')\n",
    "    ax3.set_title('User Growth')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate trend analysis\n",
    "    first_week_avg = df['avg_response_ms'].head(7).mean()\n",
    "    last_week_avg = df['avg_response_ms'].tail(7).mean()\n",
    "    percent_change = (last_week_avg - first_week_avg) / first_week_avg * 100\n",
    "    \n",
    "    print(f\"Response time trend: {percent_change:.1f}% change over {days} days\")\n",
    "    \n",
    "    user_growth = (df['daily_users'].tail(7).mean() - df['daily_users'].head(7).mean()) / df['daily_users'].head(7).mean() * 100\n",
    "    print(f\"User growth: {user_growth:.1f}% over {days} days\")\n",
    "    \n",
    "    # Make recommendation\n",
    "    if percent_change > 50 and last_week_avg > 300:\n",
    "        print(\"RECOMMENDATION: SCALE - Response times increasing significantly and exceeding SLA\")\n",
    "    elif percent_change > 30:\n",
    "        print(\"RECOMMENDATION: OPTIMIZE then consider scaling if no improvement\")\n",
    "    else:\n",
    "        print(\"RECOMMENDATION: Continue monitoring, current performance acceptable\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "response_analysis = analyze_response_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed37e5",
   "metadata": {},
   "source": [
    "## 6. Cost-Benefit Analysis\n",
    "\n",
    "Let's create a simplified cost-benefit calculator for optimization vs. scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_benefit_analysis(current_instance_cost, optimization_cost, scaling_cost, expected_optimization_gain=0.2):\n",
    "    \"\"\"Calculate ROI of optimization vs scaling\n",
    "    \n",
    "    Parameters:\n",
    "    - current_instance_cost: Monthly cost of current server\n",
    "    - optimization_cost: One-time cost of optimization effort (developer time)\n",
    "    - scaling_cost: Monthly cost of scaled infrastructure\n",
    "    - expected_optimization_gain: Expected performance improvement (0-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate monthly savings from optimization\n",
    "    effective_capacity_gain = 1 / (1 - expected_optimization_gain) - 1\n",
    "    print(f\"Effective capacity gain from optimization: {effective_capacity_gain:.1%}\")\n",
    "    \n",
    "    # Calculate break-even point for optimization\n",
    "    if scaling_cost > current_instance_cost:\n",
    "        scaling_cost_difference = scaling_cost - current_instance_cost\n",
    "        optimization_breakeven_months = optimization_cost / scaling_cost_difference\n",
    "        print(f\"Optimization breaks even in {optimization_breakeven_months:.1f} months compared to scaling\")\n",
    "    else:\n",
    "        print(\"Scaling appears to be cheaper than current instance - verify calculations\")\n",
    "    \n",
    "    # Calculate 1-year cost\n",
    "    year_cost_current = current_instance_cost * 12\n",
    "    year_cost_optimized = current_instance_cost * 12 + optimization_cost\n",
    "    year_cost_scaled = scaling_cost * 12\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Scenario': ['Current', 'Optimize', 'Scale'],\n",
    "        'Initial Cost': [0, optimization_cost, 0],\n",
    "        'Monthly Cost': [current_instance_cost, current_instance_cost, scaling_cost],\n",
    "        '1-Year Total Cost': [year_cost_current, year_cost_optimized, year_cost_scaled],\n",
    "        'Relative Capacity': [1, 1 + effective_capacity_gain, scaling_cost/current_instance_cost]  # Assuming linear scaling with cost\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: $10/month t2.micro vs $20/month t2.small vs optimization work\n",
    "analysis = cost_benefit_analysis(\n",
    "    current_instance_cost=10,  # Current server $10/month\n",
    "    optimization_cost=100,     # 5 hours optimization work at $20/hr\n",
    "    scaling_cost=20,           # Scaled server $20/month\n",
    "    expected_optimization_gain=0.3  # Expect 30% improvement from optimization\n",
    ")\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99a158",
   "metadata": {},
   "source": [
    "## 7. Practical Decision Framework for Your WatchParty App\n",
    "\n",
    "Based on everything we've covered, here's a practical decision framework:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a388e",
   "metadata": {},
   "source": [
    "### Optimize First When:\n",
    "\n",
    "1. **Memory usage** is high but **CPU usage** is moderate (<70%)\n",
    "   - Focus on memory leaks, caching strategies\n",
    "   - Adjust worker counts and memory limits\n",
    "   \n",
    "2. **Database queries** are slow or numerous\n",
    "   - Add indexes, optimize ORM usage\n",
    "   - Implement query caching\n",
    "   \n",
    "3. **Static assets** are uncompressed or not cached\n",
    "   - Implement CDN, compress assets\n",
    "   - Use proper cache headers\n",
    "   \n",
    "4. **API endpoints** are slow but not CPU-bound\n",
    "   - Implement API-level caching\n",
    "   - Optimize serialization\n",
    "\n",
    "5. **Cost is a primary concern**\n",
    "   - Optimizing existing infrastructure is almost always cheaper in the short term\n",
    "\n",
    "### Scale When:\n",
    "\n",
    "1. **CPU usage** consistently exceeds 85% despite optimizations\n",
    "   - Add more CPU cores/instances\n",
    "   \n",
    "2. **Memory usage** consistently exceeds 90% despite optimizations\n",
    "   - Add more RAM\n",
    "   \n",
    "3. **User growth** is consistent and predictable\n",
    "   - Scale ahead of projected needs\n",
    "   \n",
    "4. **Response times** continue to increase despite optimizations\n",
    "   - Indicates fundamental resource limitations\n",
    "   \n",
    "5. **New features** require additional resources\n",
    "   - Sometimes optimization isn't enough for new workloads\n",
    "\n",
    "6. **Time is more valuable than money**\n",
    "   - When developer time spent optimizing exceeds cost of scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c4f00",
   "metadata": {},
   "source": [
    "## 8. Monitoring Setup for WatchParty\n",
    "\n",
    "To make informed decisions, set up proper monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample monitoring script for key metrics\n",
    "# This is just a conceptual example - in production use proper monitoring tools like Prometheus, Grafana, etc.\n",
    "\n",
    "def monitor_health_metrics():\n",
    "    # System metrics\n",
    "    cpu = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory().percent\n",
    "    swap = psutil.swap_memory().percent\n",
    "    disk = psutil.disk_usage('/').percent\n",
    "    \n",
    "    # Sample API metrics (in production, get these from actual endpoints)\n",
    "    api_status = requests.get('http://localhost:8000/health/')\n",
    "    response_time = api_status.elapsed.total_seconds() * 1000  # Convert to ms\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'cpu_percent': cpu,\n",
    "        'memory_percent': memory,\n",
    "        'swap_percent': swap,\n",
    "        'disk_percent': disk,\n",
    "        'api_response_time_ms': response_time,\n",
    "        'api_status_code': api_status.status_code\n",
    "    }\n",
    "    \n",
    "    # In production, send to logging/monitoring system\n",
    "    print(json.dumps(metrics, indent=2))\n",
    "    \n",
    "    # Check against thresholds\n",
    "    warnings = []\n",
    "    if cpu > 85: warnings.append(f\"HIGH CPU: {cpu}%\")\n",
    "    if memory > 90: warnings.append(f\"HIGH MEMORY: {memory}%\")\n",
    "    if swap > 70: warnings.append(f\"HIGH SWAP: {swap}%\")\n",
    "    if response_time > 500: warnings.append(f\"SLOW API: {response_time:.1f}ms\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"WARNING: \" + \", \".join(warnings))\n",
    "        # In production, trigger alerts here\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "# In production, this would run on a schedule\n",
    "try:\n",
    "    current_health = monitor_health_metrics()\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring health: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7b8b8",
   "metadata": {},
   "source": [
    "## 9. Recommended Monitoring Script for WatchParty\n",
    "\n",
    "Here's a simple bash script you can use to monitor your WatchParty server and make optimization vs. scaling decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2042718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /var/www/watchparty/scripts/monitor_resources.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Simple monitoring script for WatchParty server\n",
    "# Save as /var/www/watchparty/scripts/monitor_resources.sh\n",
    "# Usage: ./monitor_resources.sh [--email admin@example.com] [--threshold 90]\n",
    "\n",
    "# Default settings\n",
    "THRESHOLD=90\n",
    "EMAIL=\"\"\n",
    "LOG_FILE=\"/var/log/watchparty/resource_usage.log\"\n",
    "ALERT_HISTORY=\"/var/log/watchparty/resource_alerts.log\"\n",
    "\n",
    "# Process command line arguments\n",
    "while [[ $# -gt 0 ]]; do\n",
    "  case $1 in\n",
    "    --email)\n",
    "      EMAIL=\"$2\"\n",
    "      shift 2\n",
    "      ;;\n",
    "    --threshold)\n",
    "      THRESHOLD=\"$2\"\n",
    "      shift 2\n",
    "      ;;\n",
    "    *)\n",
    "      echo \"Unknown option: $1\"\n",
    "      exit 1\n",
    "      ;;\n",
    "  esac\n",
    "done\n",
    "\n",
    "# Ensure log directory exists\n",
    "mkdir -p $(dirname \"$LOG_FILE\")\n",
    "mkdir -p $(dirname \"$ALERT_HISTORY\")\n",
    "\n",
    "# Get timestamp\n",
    "TIMESTAMP=$(date +\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Get resource usage\n",
    "CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | sed \"s/.*, *\\([0-9.]*\\)%* id.*/\\1/\" | awk '{print 100 - $1}')\n",
    "MEMORY_USAGE=$(free | grep Mem | awk '{print $3/$2 * 100.0}')\n",
    "SWAP_USAGE=$(free | grep Swap | awk '{if ($2 > 0) print $3/$2 * 100.0; else print 0}')\n",
    "DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')\n",
    "\n",
    "# Round to integers\n",
    "CPU_USAGE=$(printf \"%.0f\" \"$CPU_USAGE\")\n",
    "MEMORY_USAGE=$(printf \"%.0f\" \"$MEMORY_USAGE\")\n",
    "SWAP_USAGE=$(printf \"%.0f\" \"$SWAP_USAGE\")\n",
    "\n",
    "# Get load average\n",
    "LOAD_AVG=$(uptime | awk -F'load average:' '{ print $2 }' | sed 's/,.*//')\n",
    "\n",
    "# Log resource usage\n",
    "echo \"$TIMESTAMP,CPU:$CPU_USAGE%,MEM:$MEMORY_USAGE%,SWAP:$SWAP_USAGE%,DISK:$DISK_USAGE%,LOAD:$LOAD_AVG\" >> \"$LOG_FILE\"\n",
    "\n",
    "# Check for alerts\n",
    "ALERTS=\"\"\n",
    "if [ \"$CPU_USAGE\" -gt \"$THRESHOLD\" ]; then\n",
    "  ALERTS=\"$ALERTS CPU usage is high: $CPU_USAGE%\\n\"\n",
    "fi\n",
    "\n",
    "if [ \"$MEMORY_USAGE\" -gt \"$THRESHOLD\" ]; then\n",
    "  ALERTS=\"$ALERTS Memory usage is high: $MEMORY_USAGE%\\n\"\n",
    "fi\n",
    "\n",
    "if [ \"$SWAP_USAGE\" -gt \"75\" ]; then\n",
    "  ALERTS=\"$ALERTS Swap usage is high: $SWAP_USAGE%\\n\"\n",
    "fi\n",
    "\n",
    "if [ \"$DISK_USAGE\" -gt \"$THRESHOLD\" ]; then\n",
    "  ALERTS=\"$ALERTS Disk usage is high: $DISK_USAGE%\\n\"\n",
    "fi\n",
    "\n",
    "# Calculate processes by user\n",
    "PROCESS_COUNT=$(ps -eo user | sort | uniq -c | sort -nr | head -5 | tr '\\n' ' ')\n",
    "\n",
    "# Get top memory processes\n",
    "TOP_MEM_PROCESSES=$(ps -eo pid,pmem,rss,command --sort=-%mem | head -6 | tail -5 | sed 's/^/  /')\n",
    "\n",
    "# Get top CPU processes\n",
    "TOP_CPU_PROCESSES=$(ps -eo pid,pcpu,command --sort=-%cpu | head -6 | tail -5 | sed 's/^/  /')\n",
    "\n",
    "# Print summary\n",
    "echo \"===== WatchParty Resource Monitor =====\"\n",
    "echo \"Time: $TIMESTAMP\"\n",
    "echo \"CPU Usage: $CPU_USAGE%\"\n",
    "echo \"Memory Usage: $MEMORY_USAGE%\"\n",
    "echo \"Swap Usage: $SWAP_USAGE%\"\n",
    "echo \"Disk Usage: $DISK_USAGE%\"\n",
    "echo \"Load Average: $LOAD_AVG\"\n",
    "echo\n",
    "echo \"Top Memory Processes:\"\n",
    "echo \"$TOP_MEM_PROCESSES\"\n",
    "echo\n",
    "echo \"Top CPU Processes:\"\n",
    "echo \"$TOP_CPU_PROCESSES\"\n",
    "echo\n",
    "\n",
    "# Display decision guidance\n",
    "if [ \"$CPU_USAGE\" -gt 85 ] && [ \"$MEMORY_USAGE\" -gt 85 ]; then\n",
    "  echo \"RECOMMENDATION: Consider SCALING the server (both CPU and memory are high)\"\n",
    "  echo \"$TIMESTAMP - SCALING recommended (CPU: $CPU_USAGE%, MEM: $MEMORY_USAGE%)\" >> \"$ALERT_HISTORY\"\n",
    "elif [ \"$CPU_USAGE\" -gt 85 ] && [ \"$MEMORY_USAGE\" -le 70 ]; then\n",
    "  echo \"RECOMMENDATION: Consider CPU OPTIMIZATION or scaling to higher CPU instance\"\n",
    "  echo \"$TIMESTAMP - CPU OPTIMIZATION recommended (CPU: $CPU_USAGE%)\" >> \"$ALERT_HISTORY\"\n",
    "elif [ \"$MEMORY_USAGE\" -gt 85 ] && [ \"$CPU_USAGE\" -le 70 ]; then\n",
    "  echo \"RECOMMENDATION: Consider MEMORY OPTIMIZATION or scaling to higher memory instance\"\n",
    "  echo \"$TIMESTAMP - MEMORY OPTIMIZATION recommended (MEM: $MEMORY_USAGE%)\" >> \"$ALERT_HISTORY\"\n",
    "elif [ \"$CPU_USAGE\" -gt 70 ] || [ \"$MEMORY_USAGE\" -gt 70 ]; then\n",
    "  echo \"RECOMMENDATION: Monitor closely, optimize if consistent pattern observed\"\n",
    "else\n",
    "  echo \"RECOMMENDATION: Resource usage acceptable, no action needed\"\n",
    "fi\n",
    "\n",
    "# Send email alert if configured and threshold exceeded\n",
    "if [ -n \"$ALERTS\" ] && [ -n \"$EMAIL\" ]; then\n",
    "  echo -e \"WatchParty Server Alert\\n\\n$ALERTS\\nTime: $TIMESTAMP\\n\\nTop Memory Processes:\\n$TOP_MEM_PROCESSES\\n\\nTop CPU Processes:\\n$TOP_CPU_PROCESSES\" | mail -s \"WatchParty Server Alert\" \"$EMAIL\"\n",
    "  echo \"Alerts sent to $EMAIL\"\n",
    "elif [ -n \"$ALERTS\" ]; then\n",
    "  echo -e \"ALERTS:\\n$ALERTS\"\n",
    "fi\n",
    "\n",
    "# Done\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb24436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make script executable\n",
    "!chmod +x /var/www/watchparty/scripts/monitor_resources.sh\n",
    "\n",
    "# Run the script once to test\n",
    "!/var/www/watchparty/scripts/monitor_resources.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b3cb9",
   "metadata": {},
   "source": [
    "## 10. Set up Cron Job for Regular Monitoring\n",
    "\n",
    "To run the monitoring script regularly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386229b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup_cron_job.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up cron job for regular monitoring\n",
    "# This will run the monitoring script every 15 minutes\n",
    "\n",
    "# Create crontab entry\n",
    "(crontab -l 2>/dev/null; echo \"*/15 * * * * /var/www/watchparty/scripts/monitor_resources.sh >> /var/log/watchparty/monitoring_output.log 2>&1\") | crontab -\n",
    "\n",
    "echo \"Cron job set up to run every 15 minutes\"\n",
    "echo \"To add email alerts, run: /var/www/watchparty/scripts/monitor_resources.sh --email admin@example.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make setup script executable\n",
    "!chmod +x setup_cron_job.sh\n",
    "\n",
    "# Run setup script\n",
    "# Uncomment the next line to actually set up the cron job\n",
    "# !./setup_cron_job.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb76fc",
   "metadata": {},
   "source": [
    "## 11. Conclusion: The Decision Framework\n",
    "\n",
    "To summarize the decision process:\n",
    "\n",
    "1. **Monitor key metrics** consistently\n",
    "   - CPU, memory, disk usage, response times\n",
    "   \n",
    "2. **Implement optimizations first** when:\n",
    "   - Resource usage is moderate (<85%)\n",
    "   - Specific bottlenecks are identified\n",
    "   - Cost is a primary concern\n",
    "   \n",
    "3. **Scale when**:\n",
    "   - Optimizations have been exhausted\n",
    "   - Resource usage remains consistently high\n",
    "   - Growth is sustained and predictable\n",
    "   - Response times continue to degrade\n",
    "   \n",
    "4. **Consider hybrid approach**:\n",
    "   - Optimize the current workload\n",
    "   - Scale specific components as needed\n",
    "   - Use auto-scaling for predictable traffic patterns\n",
    "\n",
    "5. **Re-evaluate regularly**:\n",
    "   - User growth patterns\n",
    "   - New feature resource requirements\n",
    "   - Cost vs. performance tradeoffs\n",
    "\n",
    "By following this framework, you'll make informed decisions about when to optimize your WatchParty application versus when to scale its infrastructure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
